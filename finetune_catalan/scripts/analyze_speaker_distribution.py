"""
Script para analizar la distribuci√≥n de hablantes en datasets de Common Voice.

Ayuda a decidir la mejor estrategia: voces fijas, multi-speaker, o h√≠brido.

NOTA: Este script usa streaming mode para manejar datasets muy grandes (1M+ muestras)
sin consumir toda la RAM. El an√°lisis puede tardar unos minutos pero es memory-efficient.
"""

import argparse
from datasets import load_dataset
from collections import Counter, defaultdict
import json
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path


def parse_args():
    parser = argparse.ArgumentParser(
        description='Analizar distribuci√≥n de hablantes en datasets de catal√°n'
    )
    parser.add_argument(
        '--datasets',
        type=str,
        nargs='+',
        required=True,
        help='Lista de datasets a analizar'
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        default='./analysis',
        help='Directorio para guardar an√°lisis'
    )
    parser.add_argument(
        '--min_samples_fixed',
        type=int,
        default=100,
        help='M√≠nimo de muestras para considerar como voz fija'
    )
    parser.add_argument(
        '--min_samples_multispeaker',
        type=int,
        default=5,
        help='M√≠nimo de muestras para entrenamiento multi-speaker'
    )

    return parser.parse_args()


def get_variant_from_dataset_name(dataset_name):
    """Extrae la variante del nombre del dataset."""
    if 'central' in dataset_name.lower():
        return 'central'
    elif 'balearic' in dataset_name.lower() or 'balear' in dataset_name.lower():
        return 'balearic'
    elif 'valencian' in dataset_name.lower() or 'valencia' in dataset_name.lower():
        return 'valencian'
    elif 'northern' in dataset_name.lower() or 'nord' in dataset_name.lower():
        return 'northern'
    elif 'northwestern' in dataset_name.lower():
        return 'northwestern'
    return 'unknown'


def analyze_dataset(dataset_name, args):
    """Analiza un dataset individual."""
    print(f"\n{'='*70}")
    print(f"Analizando: {dataset_name}")
    print(f"{'='*70}")

    # Cargar dataset en modo streaming para no consumir toda la RAM
    print("Cargando dataset en modo streaming...")
    try:
        ds = load_dataset(dataset_name, split='train', streaming=True)
    except Exception as e:
        print(f"Error cargando dataset: {e}")
        return None

    # An√°lisis por hablante - procesar en streaming
    print("\nAnalizando hablantes (esto puede tomar unos minutos)...")
    speaker_counts = Counter()
    speaker_durations = defaultdict(float)
    speaker_genders = defaultdict(set)
    speaker_ages = defaultdict(set)

    total_samples = 0

    # Procesar en streaming para no cargar todo en memoria
    for i, example in enumerate(ds):
        total_samples += 1

        speaker_id = example.get('client_id', 'unknown')
        speaker_counts[speaker_id] += 1

        # Duraci√≥n (aproximada) - solo calculamos de una muestra cada 10 para ahorrar memoria
        if i % 10 == 0 and 'audio' in example and example['audio']:
            try:
                duration = len(example['audio']['array']) / example['audio']['sampling_rate']
                # Estimamos duraci√≥n multiplicando por 10 (ya que solo procesamos 1 de cada 10)
                speaker_durations[speaker_id] += duration * 10
            except:
                pass  # Saltamos si hay error en el audio

        # Metadata
        if 'gender' in example and example['gender']:
            speaker_genders[speaker_id].add(example['gender'])
        if 'age' in example and example['age']:
            speaker_ages[speaker_id].add(example['age'])

        # Mostrar progreso cada 10000 muestras
        if (i + 1) % 10000 == 0:
            print(f"  Procesadas {i + 1} muestras, {len(speaker_counts)} hablantes √∫nicos...", end='\r')

    print(f"\n  ‚úÖ Procesadas {total_samples} muestras totales")
    print(f"Total de muestras: {total_samples}")

    # Estad√≠sticas - calcular sin crear lista completa para ahorrar memoria
    total_speakers = len(speaker_counts)

    variant = get_variant_from_dataset_name(dataset_name)

    # Solo guardar top speakers para ahorrar memoria (no todos los 50k+)
    top_speakers = speaker_counts.most_common(1000)  # Solo top 1000

    # Calcular estad√≠sticas directamente desde el Counter sin crear lista completa
    counts_values = speaker_counts.values()
    min_samples = min(counts_values)
    max_samples = max(counts_values)
    total_count = sum(counts_values)
    mean_samples = total_count / total_speakers

    # Para mediana, necesitamos ordenar - limitamos a sample peque√±o
    sorted_samples = sorted(counts_values)
    median_samples = sorted_samples[len(sorted_samples)//2]

    # Calcular distribuci√≥n
    distribution_ranges = {
        '1-9': sum(1 for c in counts_values if c < 10),
        '10-49': sum(1 for c in counts_values if 10 <= c < 50),
        '50-99': sum(1 for c in counts_values if 50 <= c < 100),
        '100-199': sum(1 for c in counts_values if 100 <= c < 200),
        '200+': sum(1 for c in counts_values if c >= 200)
    }

    stats = {
        'dataset_name': dataset_name,
        'variant': variant,
        'total_samples': total_samples,
        'total_speakers': total_speakers,
        'samples_per_speaker': {
            'min': min_samples,
            'max': max_samples,
            'mean': mean_samples,
            'median': median_samples
        },
        'distribution_ranges': distribution_ranges,  # Distribuci√≥n agregada
        'speaker_counts': dict(top_speakers),  # Solo top 1000 para no consumir toda la RAM
        'speaker_durations': {k: speaker_durations[k] for k, _ in top_speakers[:100]}  # Solo top 100
    }

    # Limpiar para liberar memoria
    del sorted_samples

    # An√°lisis para estrategias
    speakers_for_fixed = [
        (speaker, count) for speaker, count in speaker_counts.items()
        if count >= args.min_samples_fixed
    ]

    speakers_for_multispeaker = [
        (speaker, count) for speaker, count in speaker_counts.items()
        if count >= args.min_samples_multispeaker
    ]

    stats['strategy_analysis'] = {
        'fixed_voices': {
            'count': len(speakers_for_fixed),
            'speakers': sorted(speakers_for_fixed, key=lambda x: x[1], reverse=True),
            'total_samples': sum(count for _, count in speakers_for_fixed)
        },
        'multispeaker': {
            'count': len(speakers_for_multispeaker),
            'total_samples': sum(count for _, count in speakers_for_multispeaker)
        }
    }

    # Imprimir resumen
    print(f"\nüìä ESTAD√çSTICAS:")
    print(f"  Total hablantes: {total_speakers}")
    print(f"  Muestras por hablante (promedio): {stats['samples_per_speaker']['mean']:.1f}")
    print(f"  Muestras por hablante (mediana): {stats['samples_per_speaker']['median']}")
    print(f"  Rango: {stats['samples_per_speaker']['min']} - {stats['samples_per_speaker']['max']}")

    print(f"\nüéØ AN√ÅLISIS DE ESTRATEGIAS:")
    print(f"  Voces fijas candidatas (>={args.min_samples_fixed} muestras):")
    print(f"    - {len(speakers_for_fixed)} hablantes")
    print(f"    - {stats['strategy_analysis']['fixed_voices']['total_samples']} muestras totales")

    if speakers_for_fixed:
        print(f"\n  Top 5 candidatos para voces fijas:")
        for i, (speaker, count) in enumerate(speakers_for_fixed[:5], 1):
            duration = speaker_durations[speaker] / 60  # minutos
            gender = list(speaker_genders[speaker])[0] if speaker_genders[speaker] else 'unknown'
            print(f"    {i}. {speaker[:10]}... : {count} muestras, {duration:.1f} min, {gender}")
    else:
        print(f"    ‚ö†Ô∏è  No hay hablantes con suficientes muestras para voces fijas")

    print(f"\n  Multi-speaker candidatos (>={args.min_samples_multispeaker} muestras):")
    print(f"    - {len(speakers_for_multispeaker)} hablantes")
    print(f"    - {stats['strategy_analysis']['multispeaker']['total_samples']} muestras totales")

    # Distribuci√≥n (ya calculada en stats['distribution_ranges'])
    print(f"\n  Distribuci√≥n de hablantes:")
    for range_name in ['1-9', '10-49', '50-99', '100-199', '200+']:
        count = stats['distribution_ranges'][range_name]
        pct = count / total_speakers * 100
        print(f"    {range_name} muestras: {count} hablantes ({pct:.1f}%)")

    return stats


def create_visualizations(all_stats, output_dir):
    """Crea visualizaciones del an√°lisis."""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    try:
        import matplotlib
        matplotlib.use('Agg')  # Backend sin GUI

        # Gr√°fico de distribuci√≥n de muestras por hablante
        for stats in all_stats:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f"An√°lisis: {stats['variant']}", fontsize=16)

            samples = list(stats['speaker_counts'].values())

            # Histograma
            axes[0, 0].hist(samples, bins=50, edgecolor='black')
            axes[0, 0].set_xlabel('Muestras por hablante')
            axes[0, 0].set_ylabel('N√∫mero de hablantes')
            axes[0, 0].set_title('Distribuci√≥n de muestras')
            axes[0, 0].axvline(100, color='r', linestyle='--', label='Umbral voces fijas')
            axes[0, 0].legend()

            # Top hablantes
            top_20 = sorted(stats['speaker_counts'].items(), key=lambda x: x[1], reverse=True)[:20]
            speakers, counts = zip(*top_20)
            speakers_short = [s[:10] + '...' for s in speakers]
            axes[0, 1].barh(range(len(speakers_short)), counts)
            axes[0, 1].set_yticks(range(len(speakers_short)))
            axes[0, 1].set_yticklabels(speakers_short, fontsize=8)
            axes[0, 1].set_xlabel('Muestras')
            axes[0, 1].set_title('Top 20 hablantes')
            axes[0, 1].invert_yaxis()

            # Distribuci√≥n acumulativa (solo top 1000 para evitar OOM)
            sorted_samples = sorted(samples, reverse=True)[:1000]  # Limitar a top 1000
            cumsum = [sum(sorted_samples[:i+1]) for i in range(len(sorted_samples))]
            axes[1, 0].plot(cumsum)
            axes[1, 0].set_xlabel('N√∫mero de hablantes (top 1000)')
            axes[1, 0].set_ylabel('Muestras acumuladas')
            axes[1, 0].set_title('Muestras acumuladas por top hablantes')
            axes[1, 0].grid(True)

            # Resumen de estad√≠sticas
            axes[1, 1].axis('off')
            summary_text = f"""
Resumen Estad√≠stico:

Total hablantes: {stats['total_speakers']}
Total muestras: {stats['total_samples']}

Muestras por hablante:
  ‚Ä¢ Promedio: {stats['samples_per_speaker']['mean']:.1f}
  ‚Ä¢ Mediana: {stats['samples_per_speaker']['median']}
  ‚Ä¢ Min-Max: {stats['samples_per_speaker']['min']}-{stats['samples_per_speaker']['max']}

Candidatos para voces fijas: {stats['strategy_analysis']['fixed_voices']['count']}
Candidatos para multi-speaker: {stats['strategy_analysis']['multispeaker']['count']}
            """
            axes[1, 1].text(0.1, 0.5, summary_text, fontsize=12, family='monospace')

            plt.tight_layout()
            output_path = output_dir / f"analysis_{stats['variant']}.png"
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            plt.close()

            print(f"Visualizaci√≥n guardada: {output_path}")

    except ImportError:
        print("matplotlib no disponible, saltando visualizaciones")


def generate_report(all_stats, args, output_dir):
    """Genera reporte en markdown."""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    report_path = output_dir / 'speaker_analysis_report.md'

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# An√°lisis de Distribuci√≥n de Hablantes\n\n")
        f.write(f"Fecha: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\n\n")

        # Resumen general
        f.write("## Resumen General\n\n")
        total_samples = sum(s['total_samples'] for s in all_stats)
        total_speakers = sum(s['total_speakers'] for s in all_stats)
        f.write(f"- **Total de datasets analizados**: {len(all_stats)}\n")
        f.write(f"- **Total de muestras**: {total_samples:,}\n")
        f.write(f"- **Total de hablantes**: {total_speakers:,}\n\n")

        # Por dataset
        f.write("## An√°lisis por Dataset\n\n")
        for stats in all_stats:
            f.write(f"### {stats['variant'].capitalize()}\n\n")
            f.write(f"**Dataset**: `{stats['dataset_name']}`\n\n")

            f.write("#### Estad√≠sticas B√°sicas\n\n")
            f.write(f"- Total hablantes: {stats['total_speakers']}\n")
            f.write(f"- Total muestras: {stats['total_samples']}\n")
            f.write(f"- Promedio muestras/hablante: {stats['samples_per_speaker']['mean']:.1f}\n")
            f.write(f"- Mediana muestras/hablante: {stats['samples_per_speaker']['median']}\n\n")

            f.write("#### Recomendaci√≥n de Estrategia\n\n")
            fixed_count = stats['strategy_analysis']['fixed_voices']['count']
            multi_count = stats['strategy_analysis']['multispeaker']['count']

            if fixed_count >= 3:
                f.write(f"‚úÖ **Estrategia H√≠brida recomendada**\n")
                f.write(f"- {fixed_count} hablantes con suficientes datos para voces fijas\n")
                f.write(f"- {multi_count} hablantes para entrenamiento multi-speaker\n\n")
            elif multi_count >= 50:
                f.write(f"‚úÖ **Estrategia Multi-Speaker recomendada**\n")
                f.write(f"- {multi_count} hablantes con datos suficientes\n")
                f.write(f"- Ideal para zero-shot voice cloning\n\n")
            else:
                f.write(f"‚ö†Ô∏è  **Dataset peque√±o**\n")
                f.write(f"- Considerar usar modelo preentrenado con zero-shot\n\n")

            # Top hablantes
            if stats['strategy_analysis']['fixed_voices']['speakers']:
                f.write("#### Top Candidatos para Voces Fijas\n\n")
                f.write("| Hablante | Muestras | % del Total |\n")
                f.write("|----------|----------|-------------|\n")
                for speaker, count in stats['strategy_analysis']['fixed_voices']['speakers'][:10]:
                    pct = count / stats['total_samples'] * 100
                    f.write(f"| {speaker[:15]}... | {count} | {pct:.1f}% |\n")
                f.write("\n")

        # Recomendaciones finales
        f.write("## Recomendaciones Finales\n\n")

        total_fixed_speakers = sum(
            s['strategy_analysis']['fixed_voices']['count'] for s in all_stats
        )
        total_multi_speakers = sum(
            s['strategy_analysis']['multispeaker']['count'] for s in all_stats
        )

        f.write(f"### Estrategia Recomendada\n\n")

        if total_fixed_speakers >= 5:
            f.write("**Estrategia H√≠brida**: Combinar voces fijas + multi-speaker\n\n")
            f.write("1. Seleccionar top 2-3 hablantes por dialecto para voces fijas\n")
            f.write("2. Usar el resto para entrenamiento multi-speaker\n")
            f.write("3. Resultado: Voces de alta calidad + flexibilidad zero-shot\n\n")
        else:
            f.write("**Estrategia Multi-Speaker**: Entrenamiento para zero-shot cloning\n\n")
            f.write("1. Usar todos los hablantes (min 5 muestras cada uno)\n")
            f.write("2. Entrenar con in-context learning\n")
            f.write("3. Resultado: Capacidad de clonar cualquier voz catalana\n\n")

        f.write("### Comandos Sugeridos\n\n")
        f.write("```bash\n")
        f.write("# Para voces fijas (si aplica)\n")
        f.write("python scripts/prepare_fixed_voices.py \\\n")
        f.write(f"    --min_samples_per_speaker {args.min_samples_fixed} \\\n")
        f.write("    --speakers_per_dialect 2\n\n")
        f.write("# Para multi-speaker\n")
        f.write("python scripts/prepare_multispeaker_catalan.py \\\n")
        f.write(f"    --min_samples_per_speaker {args.min_samples_multispeaker} \\\n")
        f.write("    --max_samples_per_speaker 50\n")
        f.write("```\n")

    print(f"\n‚úÖ Reporte generado: {report_path}")


def main():
    args = parse_args()

    print("="*70)
    print("AN√ÅLISIS DE DISTRIBUCI√ìN DE HABLANTES")
    print("="*70)

    all_stats = []

    for dataset_name in args.datasets:
        stats = analyze_dataset(dataset_name, args)
        if stats:
            all_stats.append(stats)

    if not all_stats:
        print("\n‚ùå No se pudo analizar ning√∫n dataset")
        return

    # Guardar estad√≠sticas
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    stats_path = output_dir / 'speaker_statistics.json'
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(all_stats, f, indent=2, ensure_ascii=False)

    print(f"\n‚úÖ Estad√≠sticas guardadas: {stats_path}")

    # Crear visualizaciones
    create_visualizations(all_stats, output_dir)

    # Generar reporte
    generate_report(all_stats, args, output_dir)

    print("\n" + "="*70)
    print("AN√ÅLISIS COMPLETADO")
    print("="*70)


if __name__ == '__main__':
    main()
